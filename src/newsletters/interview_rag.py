"""
RAG implementation for Interview System using PydanticAI
*generated by Claude 3.5 Sonnet

This module implements a Retrieval-Augmented Generation (RAG) system
adapted from the PydanticAI example to work with interview data and insights.

The system:
1. Embeds interview transcripts and insights into a vector database
2. Provides semantic search capabilities for interview content
3. Uses a PydanticAI agent to answer questions about organizational insights
"""

from __future__ import annotations

import asyncio
import re
import unicodedata
from contextlib import asynccontextmanager
from dataclasses import dataclass
from datetime import datetime
from typing import Optional
from uuid import UUID, uuid4

import asyncpg
import logfire
import pydantic_core
from openai import AsyncOpenAI
from pydantic import BaseModel, Field
from typing_extensions import AsyncGenerator

from pydantic_ai import RunContext
from pydantic_ai.agent import Agent

# Configure logfire for monitoring
logfire.configure(send_to_logfire='if-token-present')
logfire.instrument_asyncpg()
logfire.instrument_pydantic_ai()


# Pydantic models for our interview system
class Person(BaseModel):
    """Model representing a person in the organization"""
    id: UUID = Field(default_factory=uuid4)
    name: str
    email: str
    team: str
    role: str
    manager_id: Optional[UUID] = None


class InterviewMetadata(BaseModel):
    """Metadata for an interview"""
    duration_minutes: Optional[float] = None
    recording_quality: Optional[str] = None
    transcription_source: Optional[str] = None
    privacy_level: str = "internal"
    tags: list[str] = Field(default_factory=list)


class QuestionResponse(BaseModel):
    """A single question and response in an interview"""
    question_id: str
    question_text: str
    response_text: str
    response_sentiment: Optional[float] = None
    response_themes: list[str] = Field(default_factory=list)


class Interview(BaseModel):
    """Complete interview model"""
    id: UUID = Field(default_factory=uuid4)
    timestamp: datetime
    interviewer: Person
    interviewee: Person
    context: str  # e.g., "peer_review", "manager_checkin", "pulse_check"
    questions: list[QuestionResponse]
    metadata: InterviewMetadata
    transcription: Optional[str] = None
    sentiment_analysis: Optional[float] = None
    missing_fields: list[str] = Field(default_factory=list)


@dataclass
class RAGDependencies:
    """Dependencies for the RAG agent"""
    openai: AsyncOpenAI
    pool: asyncpg.Pool


# Create the PydanticAI agent for interview insights
agent = Agent('openai:gpt-4o', deps_type=RAGDependencies)


@agent.tool
async def retrieve_interviews(context: RunContext[RAGDependencies], search_query: str) -> str:
    """
    Retrieve interview content based on a search query.

    This tool searches through interview transcripts, responses, and metadata
    to find relevant content that can help answer questions about organizational insights.

    Args:
        context: The call context with dependencies
        search_query: The search query for finding relevant interviews
    """
    with logfire.span('create embedding for search query', search_query=search_query):
        embedding = await context.deps.openai.embeddings.create(
            input=search_query,
            model='text-embedding-3-small',
        )

    assert len(embedding.data) == 1, (
        f'Expected 1 embedding, got {len(embedding.data)}, search query: {search_query!r}'
    )

    embedding_vector = embedding.data[0].embedding
    embedding_json = pydantic_core.to_json(embedding_vector).decode()

    # Search for similar interview content
    rows = await context.deps.pool.fetch(
        '''
        SELECT 
            interview_id,
            interviewee_name,
            interviewee_team,
            interview_context,
            content,
            timestamp,
            sentiment
        FROM interview_embeddings 
        ORDER BY embedding <-> $1 
        LIMIT 10
        ''',
        embedding_json,
    )

    if not rows:
        return "No relevant interview content found for the search query."

    results = []
    for row in rows:
        results.append(
            f"## Interview with {row['interviewee_name']} ({row['interviewee_team']} team)\n"
            f"Date: {row['timestamp']}\n"
            f"Context: {row['interview_context']}\n"
            f"Sentiment: {row['sentiment']}\n\n"
            f"{row['content']}\n"
        )

    return '\n\n---\n\n'.join(results)


@agent.tool
async def get_team_sentiment_trends(context: RunContext[RAGDependencies], team_name: str, days_back: int = 30) -> str:
    """
    Get sentiment trends for a specific team over time.

    Args:
        context: The call context
        team_name: Name of the team to analyze
        days_back: Number of days to look back (default 30)
    """
    rows = await context.deps.pool.fetch(
        '''
        SELECT 
            DATE(timestamp) as interview_date,
            AVG(sentiment) as avg_sentiment,
            COUNT(*) as interview_count
        FROM interview_embeddings 
        WHERE interviewee_team = $1 
        AND timestamp >= NOW() - INTERVAL '%s days'
        GROUP BY DATE(timestamp)
        ORDER BY interview_date DESC
        ''',
        team_name,
        days_back
    )

    if not rows:
        return f"No interview data found for team '{team_name}' in the last {days_back} days."

    trend_data = []
    for row in rows:
        trend_data.append(
            f"Date: {row['interview_date']}, "
            f"Average Sentiment: {row['avg_sentiment']:.2f}, "
            f"Interviews: {row['interview_count']}"
        )

    return f"Sentiment trends for {team_name} team:\n" + '\n'.join(trend_data)


@agent.tool
async def find_recurring_issues(context: RunContext[RAGDependencies], theme: str) -> str:
    """
    Find interviews that mention recurring issues or specific themes.

    Args:
        context: The call context
        theme: The theme or issue to search for (e.g., "deployment", "communication", "tools")
    """
    # Use semantic search to find content related to the theme
    with logfire.span('create embedding for theme search', theme=theme):
        embedding = await context.deps.openai.embeddings.create(
            input=f"frustration problem issue {theme}",
            model='text-embedding-3-small',
        )

    embedding_vector = embedding.data[0].embedding
    embedding_json = pydantic_core.to_json(embedding_vector).decode()

    rows = await context.deps.pool.fetch(
        '''
        SELECT 
            interviewee_name,
            interviewee_team,
            content,
            timestamp,
            sentiment
        FROM interview_embeddings 
        WHERE embedding <-> $1 < 0.3  -- Similarity threshold
        ORDER BY embedding <-> $1 
        LIMIT 15
        ''',
        embedding_json,
    )

    if not rows:
        return f"No recurring issues found related to '{theme}'."

    results = []
    for row in rows:
        results.append(
            f"**{row['interviewee_name']}** ({row['interviewee_team']}) - {row['timestamp'].strftime('%Y-%m-%d')}:\n"
            f"{row['content'][:200]}...\n"
            f"Sentiment: {row['sentiment']}\n"
        )

    return f"Recurring issues related to '{theme}':\n\n" + '\n\n'.join(results)


async def run_interview_agent(question: str) -> str:
    """
    Entry point to run the interview insights agent.

    Args:
        question: Question about organizational insights from interviews

    Returns:
        Agent's response based on interview data
    """
    openai = AsyncOpenAI()
    logfire.instrument_openai(openai)

    logfire.info('Asking interview question', question=question)

    async with database_connect(False) as pool:
        deps = RAGDependencies(openai=openai, pool=pool)
        result = await agent.run(question, deps=deps)
        return result.output


# Database management functions
@asynccontextmanager
async def database_connect(create_db: bool = False) -> AsyncGenerator[asyncpg.Pool, None]:
    """Connect to the interview database"""
    server_dsn = 'postgresql://postgres:postgres@localhost:54320'
    database = 'interview_insights'

    if create_db:
        with logfire.span('check and create interview database'):
            conn = await asyncpg.connect(server_dsn)
            try:
                db_exists = await conn.fetchval(
                    'SELECT 1 FROM pg_database WHERE datname = $1', database
                )
                if not db_exists:
                    await conn.execute(f'CREATE DATABASE {database}')
            finally:
                await conn.close()

    pool = await asyncpg.create_pool(f'{server_dsn}/{database}')
    try:
        yield pool
    finally:
        await pool.close()

# Creates a coroutine
async def setup_database():
    """Set up the interview database schema"""
    async with database_connect(True) as pool:
        with logfire.span('create interview schema'):
            async with pool.acquire() as conn:
                async with conn.transaction():
                    await conn.execute(DB_SCHEMA)


async def store_interview_embedding(interview: Interview) -> None:
    """
    Store an interview with its embedding in the database.

    Args:
        interview: The interview object to store
    """
    openai = AsyncOpenAI()

    # Create content for embedding
    content_parts = []
    content_parts.append(f"Interview context: {interview.context}")
    content_parts.append(
        f"Interviewer: {interview.interviewer.name} ({interview.interviewer.role})")
    content_parts.append(
        f"Interviewee: {interview.interviewee.name} ({interview.interviewee.role})")

    for qa in interview.questions:
        content_parts.append(f"Q: {qa.question_text}")
        content_parts.append(f"A: {qa.response_text}")

    if interview.transcription:
        content_parts.append(f"Full transcription: {interview.transcription}")

    embedding_content = '\n\n'.join(content_parts)

    # Generate embedding
    with logfire.span('create embedding for interview', interview_id=str(interview.id)):
        embedding = await openai.embeddings.create(
            input=embedding_content,
            model='text-embedding-3-small',
        )

    embedding_vector = embedding.data[0].embedding
    embedding_json = pydantic_core.to_json(embedding_vector).decode()

    # Store in database
    async with database_connect(False) as pool:
        await pool.execute(
            '''
            INSERT INTO interview_embeddings 
            (interview_id, interviewee_name, interviewee_team, interview_context, 
             content, timestamp, sentiment, embedding)
            VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
            ON CONFLICT (interview_id) DO UPDATE SET
                content = EXCLUDED.content,
                sentiment = EXCLUDED.sentiment,
                embedding = EXCLUDED.embedding
            ''',
            str(interview.id),
            interview.interviewee.name,
            interview.interviewee.team,
            interview.context,
            embedding_content,
            interview.timestamp,
            interview.sentiment_analysis or 0.0,
            embedding_json,
        )


# Database schema
DB_SCHEMA = """
CREATE EXTENSION IF NOT EXISTS vector;

CREATE TABLE IF NOT EXISTS interview_embeddings (
    id SERIAL PRIMARY KEY,
    interview_id UUID NOT NULL UNIQUE,
    interviewee_name TEXT NOT NULL,
    interviewee_team TEXT NOT NULL,
    interview_context TEXT NOT NULL,
    content TEXT NOT NULL,
    timestamp TIMESTAMP WITH TIME ZONE NOT NULL,
    sentiment FLOAT,
    -- text-embedding-3-small returns a vector of 1536 floats
    embedding vector(1536) NOT NULL
);

CREATE INDEX IF NOT EXISTS idx_interview_embeddings_vector 
ON interview_embeddings USING hnsw (embedding vector_l2_ops);

CREATE INDEX IF NOT EXISTS idx_interview_embeddings_team 
ON interview_embeddings (interviewee_team);

CREATE INDEX IF NOT EXISTS idx_interview_embeddings_timestamp 
ON interview_embeddings (timestamp);

CREATE INDEX IF NOT EXISTS idx_interview_embeddings_context 
ON interview_embeddings (interview_context);
"""


def slugify(value: str, separator: str = '-', unicode: bool = False) -> str:
    """Slugify a string to make it URL friendly"""
    if not unicode:
        value = unicodedata.normalize('NFKD', value)
        value = value.encode('ascii', 'ignore').decode('ascii')
    value = re.sub(r'[^\w\s-]', '', value).strip().lower()
    return re.sub(rf'[{separator}\s]+', separator, value)


# Example usage functions
async def create_sample_interviews() -> None:
    """Create some sample interviews for testing"""
    from datetime import datetime, timedelta

    # Sample people
    alice = Person(name="Alice Johnson", email="alice@company.com",
                   team="Engineering", role="Senior Developer")
    bob = Person(name="Bob Smith", email="bob@company.com",
                 team="Engineering", role="Team Lead")
    carol = Person(name="Carol Davis", email="carol@company.com",
                   team="Design", role="UX Designer")

    # Sample interviews
    interviews = [
        Interview(
            timestamp=datetime.now() - timedelta(days=1),
            interviewer=bob,
            interviewee=alice,
            context="peer_review",
            questions=[
                QuestionResponse(
                    question_id="1",
                    question_text="What's your biggest win this week?",
                    response_text="I finally got the new deployment pipeline working smoothly. It's reduced our build times by 50%.",
                    response_sentiment=0.8
                ),
                QuestionResponse(
                    question_id="2",
                    question_text="What's been frustrating lately?",
                    response_text="The CI/CD system keeps failing randomly. It's really slowing down our development process.",
                    response_sentiment=-0.6
                ),
            ],
            metadata=InterviewMetadata(duration_minutes=15.0, tags=[
                                       "engineering", "deployment"]),
            sentiment_analysis=0.1
        ),
        Interview(
            timestamp=datetime.now() - timedelta(days=2),
            interviewer=alice,
            interviewee=carol,
            context="cross_team",
            questions=[
                QuestionResponse(
                    question_id="1",
                    question_text="How's collaboration between our teams?",
                    response_text="It's been really good! The design system we built together is making everything much more consistent.",
                    response_sentiment=0.7
                ),
                QuestionResponse(
                    question_id="2",
                    question_text="Any blockers we should know about?",
                    response_text="Sometimes the handoff process could be smoother. We need better documentation.",
                    response_sentiment=-0.2
                ),
            ],
            metadata=InterviewMetadata(duration_minutes=20.0, tags=[
                                       "cross-team", "design"]),
            sentiment_analysis=0.25
        ),
    ]

    # Store the sample interviews
    for interview in interviews:
        await store_interview_embedding(interview)

    print(f"Created {len(interviews)} sample interviews")


if __name__ == '__main__':
    import sys

    if len(sys.argv) > 1:
        action = sys.argv[1]

        if action == 'setup':
            asyncio.run(setup_database())
            print("Database schema created successfully!")

        elif action == 'sample':
            asyncio.run(setup_database())
            asyncio.run(create_sample_interviews())
            print("Sample interviews created!")

        elif action == 'ask':
            question = sys.argv[2] if len(
                sys.argv) > 2 else "What are the main frustrations people are experiencing?"
            result = asyncio.run(run_interview_agent(question))
            print(f"\n🤖 Answer: {result}")

        else:
            print(
                "Usage: python interview_rag.py [setup|sample|ask] [question]")
    else:
        print("Usage: python interview_rag.py [setup|sample|ask] [question]")
        print("  setup  - Create database schema")
        print("  sample - Create sample interview data")
        print("  ask    - Ask a question about interviews")
